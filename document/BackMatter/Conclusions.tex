\begin{conclusions}
    \label{sec:19}
        
            La arquitectura propuesta más que un algortimo optimizador es un marco de trabajo que generaliza el concepto de perceptron, puesto que en lugar de estimar 
            hiper parámetros muy complicados de hallar, se ofrece una alternativa para minimizar el esfuerzo que representa tal ardua tarea desde el número de neuronas y 
            capas ocultas hasta la función de activación, además de ser altamente paralelizable, lo cual es un estándar entre los algoritmos de aprendizaje de máquinas de 
            hoy en día. Podemos concluir que el algoritmo propuesto es convergente condicionalemente, y tal condición la da el agente coordinador, se han explorado 
            el enfoque de intervalos para representar incertidumbre, se han definido las restricciones del lenguaje necesarios para que los agentes se comunique entre sí y 
            minimizen la función objetivo y por último se implementó un prototipo básico de uso.    
\end{conclusions}
